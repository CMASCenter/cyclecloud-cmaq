Command:        mpirun -np 96 /shared/build/openmpi_gcc/CMAQ_v54+/CCTM/scripts/BLD_CCTM_v54_gcc/CCTM_v54.exe
Resources:      1 node (120 physical, 120 logical cores per node)
Memory:         441 GiB per node
Tasks:          96 processes
Machine:        cyclecloudlizadams-hpc-pg0-2
Start time:     Mon Dec 5 17:55:11 2022
Total time:     333 seconds (about 6 minutes)
Full path:      /shared/build/openmpi_gcc/CMAQ_v54+/CCTM/scripts/BLD_CCTM_v54+_gcc

Summary: CCTM_v54+.exe is Compute-bound in this configuration
Compute:                                     49.6% |====|
MPI:                                         35.0% |==|
I/O:                                         15.4% |=|
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU section below. 

CPU:
A breakdown of the 49.6% CPU time:
Scalar numeric ops:                          33.9% |==|
Vector numeric ops:                          10.4% ||
Memory accesses:                             55.7% |=====|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
Little time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 35.0% MPI time:
Time in collective calls:                    81.8% |=======|
Time in point-to-point calls:                18.2% |=|
Effective process collective rate:            22.6 kB/s
Effective process point-to-point rate:         119 MB/s
Most of the time is spent in collective calls with a very low transfer rate. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.

I/O:
A breakdown of the 15.4% I/O time:
Time in reads:                               71.9% |======|
Time in writes:                              28.1% |==|
Effective process read rate:                  4.18 GB/s
Effective process write rate:                 5.16 MB/s
Most of the time is spent in read operations with a high effective transfer rate. It may be possible to achieve faster effective transfer rates using asynchronous file operations.

Threads:
A breakdown of how multiple threads were used:
Computation:                                100.0% |=========|
Synchronization:                             <0.1% ||
Physical core utilization:                   67.6% |======|
System load:                                 76.0% |=======|
Physical core utilization is low. Try increasing the number of threads or processes to improve performance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                     807 MiB
Peak process memory usage:                    1.24 GiB
Peak node memory usage:                      20.0% |=|
There is significant variation between peak and mean memory usage. This may be a sign of workload imbalance or a memory leak.
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
Energy metrics are not available on this system.
CPU metrics are not supported (no intel_rapl module)

